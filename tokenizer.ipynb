{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a92950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb539e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/home/shivraj-pg/DEPNECT/conllu-style-csv/without-context-coarse-train.csv\")\n",
    "df2 = pd.read_csv(\n",
    "    \"/home/shivraj-pg/DEPNECT/conllu-style-csv/without-context-coarse-test.csv\")\n",
    "df3 = pd.read_csv(\n",
    "    \"/home/shivraj-pg/DEPNECT/conllu-style-csv/without-context-coarse-dev.csv\")\n",
    "\n",
    "merged = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f130c1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11000, 2), (2940, 2), (2000, 2), (15940, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape, df2.shape, df3.shape, merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc698c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[['sentence', 'gold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b34c701b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'gold'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061708da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'gold'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = merged[['sentence', 'gold']]\n",
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0c3d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 15940/15940 [00:08<00:00, 1795.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-27b-it\")\n",
    "\n",
    "# Columns\n",
    "gold_col = \"gold\"\n",
    "gold_tokens_col = \"gold-tokens\"\n",
    "gold_length_col = \"gold-token-len\"\n",
    "\n",
    "sent_col = \"sentence\"\n",
    "sent_tokens_col = \"sent-tokens\"\n",
    "sent_length_col = \"sent-token-len\"\n",
    "\n",
    "# Ensure columns exist\n",
    "for col, default in [\n",
    "    (gold_tokens_col, None),\n",
    "    (gold_length_col, 0),\n",
    "    (sent_tokens_col, None),\n",
    "    (sent_length_col, 0),\n",
    "]:\n",
    "    if col not in merged.columns:\n",
    "        if default is None:\n",
    "            merged[col] = pd.Series([None] * len(merged), dtype=object)\n",
    "        else:\n",
    "            merged[col] = default\n",
    "\n",
    "# Helper function\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [], 0\n",
    "    tokens = tokenizer.encode(text.strip(), add_special_tokens=False)\n",
    "    return tokens, len(tokens)\n",
    "\n",
    "# Tokenization loop\n",
    "for idx, row in tqdm(merged.iterrows(), total=len(merged), desc=\"Tokenizing\"):\n",
    "    gold_tokens, gold_len = tokenize_text(row[gold_col])\n",
    "    merged.at[idx, gold_tokens_col] = gold_tokens\n",
    "    merged.at[idx, gold_length_col] = gold_len\n",
    "\n",
    "    sent_tokens, sent_len = tokenize_text(row[sent_col])\n",
    "    merged.at[idx, sent_tokens_col] = sent_tokens\n",
    "    merged.at[idx, sent_length_col] = sent_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15dddd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'gold', 'gold-tokens', 'gold-token-len', 'sent-tokens',\n",
       "       'sent-token-len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef24f629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent-token-len</th>\n",
       "      <th>gold-token-len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15940.000000</td>\n",
       "      <td>15940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.938645</td>\n",
       "      <td>478.811104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.602983</td>\n",
       "      <td>223.114830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>305.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>432.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1554.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sent-token-len  gold-token-len\n",
       "count    15940.000000    15940.000000\n",
       "mean        27.938645      478.811104\n",
       "std         12.602983      223.114830\n",
       "min          6.000000      118.000000\n",
       "25%         19.000000      305.000000\n",
       "50%         25.000000      432.000000\n",
       "75%         34.000000      599.000000\n",
       "max         90.000000     1554.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[['sent-token-len', 'gold-token-len']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "049b801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "You are an expert in Sanskrit grammar, who identifies and classifies compounds in the given Sanskrit sentence. You will be given the original sentence. First break the sentence in compounds.\n",
    "Follow these rules strictly:\n",
    "1. Only use the following 4 compound types. Do not invent or include other types:\n",
    "    - Tatpurusha: An endocentric compound where the first element (the attributive) determines the second.\n",
    "    - Avyayibhava: An adverbial compound made of an indeclinable element and a noun, expressing an adverbial meaning.\n",
    "    - Dvandva: A copulative compound where two or more noun stems are joined by 'and'.\n",
    "    - Bahuvrihi: An exocentric compound that describes something by referring to its parts.\n",
    "2. The sentence may contain nested compounds or non-compounded words — handle appropriately.\n",
    "3. Maintain strict formatting and provide only the answer line. Do not include explanations.\n",
    "4. The start or end indexes must not exceed the number of words in the sentence.\n",
    "5. Answer in the devnagri script only, there shouldn't be any latin in the answer\n",
    "\n",
    "Text:\n",
    "{INPUT}\n",
    "\n",
    "Return strictly in JSON with keys:\n",
    "{\n",
    "  \"tokens\": [...],\n",
    "  \"compounds\": [\n",
    "    {\n",
    "      \"span\": [start_token_index, end_token_index],\n",
    "      \"label\": \"<Samasa_type>\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Tokenize by meaningful Sanskrit units.\n",
    "- span = inclusive of start index, exclusive of end index.\n",
    "- If multiple nested samāsa exist, include all.\n",
    "- If none, return empty lists for compounds.\n",
    "- Do not output anything outside JSON.\n",
    "\n",
    "\n",
    "Example 1:\n",
    "Input: ससर्षपंतुम्बुरुधान्यवन्यंचण्डांचचूर्णानिसमानिकुर्यात्DUMMY\n",
    "Output:\n",
    "{'tokens': ['स', 'सर्षपं', 'तुम्बुरु', 'धान्य', 'वन्यं', 'चण्डां', 'च', 'चूर्णानि', 'समानि', 'कुर्यात्', 'DUMMY'], \n",
    "'compounds': [\n",
    "    {'span': ['1', '2'], 'label': 'Bahuvrihi'}, \n",
    "    {'span': ['2', '11'], 'label': 'Comp_root'}, \n",
    "    {'span': ['3', '5'], 'label': 'Dvandva'}, \n",
    "    {'span': ['4', '5'], 'label': 'Dvandva'}, \n",
    "    {'span': ['5', '11'], 'label': 'Comp_root'}, \n",
    "    {'span': ['6', '11'], 'label': 'No_rel'}, \n",
    "    {'span': ['7', '11'], 'label': 'No_rel'}, \n",
    "    {'span': ['8', '11'], 'label': 'No_rel'}, \n",
    "    {'span': ['9', '11'], 'label': 'No_rel'}, \n",
    "    {'span': ['10', '11'], 'label': 'No_rel'}, \n",
    "    {'span': ['11', '0'], 'label': 'root'}]}\n",
    "   \n",
    "Example 2:\n",
    "Input: आपाततसामान्याइवप्रतीयमानाएतेयदिसूक्ष्मम्निरीक्ष्येरन्तर्हिएतेषाम्हृत्अन्तस्थसंकटबोधDUMMY\n",
    "Output:\n",
    "{'tokens': ['आपातत', 'सामान्या', 'इव', 'प्रतीयमाना', 'एते', 'यदि', 'सूक्ष्मम्', 'निरीक्ष्येरन्', 'तर्हि', 'एतेषाम्', 'हृत्', 'अन्त', 'स्थ', 'संकट', 'बोध', 'DUMMY'], \n",
    "'compounds': [\n",
    "    {'span': ['1', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['2', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['3', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['4', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['5', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['6', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['7', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['8', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['9', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['10', '16'], 'label': 'No_rel'}, \n",
    "    {'span': ['11', '12'], 'label': 'Tatpurusha'}, \n",
    "    {'span': ['12', '13'], 'label': 'Tatpurusha'}, \n",
    "    {'span': ['13', '16'], 'label': 'Comp_root'}, \n",
    "    {'span': ['14', '15'], 'label': 'Tatpurusha'}, \n",
    "    {'span': ['15', '16'], 'label': 'Comp_root'}, \n",
    "    {'span': ['16', '0'], 'label': 'root'}]}\n",
    "    \n",
    "Input: सःचनविशिष्टवैशिष्ट्यअवगाहीDUMMY\n",
    "Output:\n",
    "{'tokens': ['सः', 'च', 'न', 'विशिष्ट', 'वैशिष्ट्य', 'अवगाही', 'DUMMY'], \n",
    "'compounds': [\n",
    "    {'span': ['1', '7'], 'label': 'No_rel'}, \n",
    "    {'span': ['2', '7'], 'label': 'No_rel'}, \n",
    "    {'span': ['3', '7'], 'label': 'No_rel'}, \n",
    "    {'span': ['4', '5'], 'label': 'Tatpurusha'}, \n",
    "    {'span': ['5', '6'], 'label': 'Tatpurusha'}, \n",
    "    {'span': ['6', '7'], 'label': 'Comp_root'}, \n",
    "    {'span': ['7', '0'], 'label': 'root'}]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69ac030",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer.encode(SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "536d1c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1370"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aff70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'gold'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/shivraj-pg/DEPNECT/conllu-style-csv/asthangrudyam.csv\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5b350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('रागआदिरोगान् सततअनुषक्तान् अशेषकायप्रसृतान् अशेषान् औत्सुक्यमोहअरतिदान् जघान (यः) .',\n",
       " \"{'tokens': ['राग', 'आदि', 'रोगान्', 'सतत', 'अनुषक्तान्', 'अ', 'शेष', 'काय', 'प्रसृतान्', 'अ', 'शेषान्', 'औत्सुक्य', 'मोह', 'अ', 'रति', 'दान्', 'जघान', '(यः)', '.'], 'compounds': '1\\\\tराग-\\\\tराग-आदि-रोगान्\\\\t2\\\\tComp3\\\\t_\\\\tबहुव्रीहिः\\\\n2\\\\tआदि-\\\\t--\\\\t3\\\\tComp3\\\\t_\\\\tकर्मधारयः\\\\n3\\\\tरोगान्\\\\t--\\\\t20\\\\tComp3\\\\t_\\\\tComp_root\\\\n4\\\\tसतत-\\\\tसतत-अनुषक्तान्\\\\t5\\\\tComp2\\\\t_\\\\tकर्मधारयः\\\\n5\\\\tअनुषक्तान्\\\\t--\\\\t20\\\\tComp2\\\\t_\\\\tComp_root\\\\n6\\\\tअ-\\\\tअ-शेष-काय-प्रसृतान्\\\\t7\\\\tComp4\\\\t_\\\\tनञ्-तत्पुरुषः\\\\n7\\\\tशेष-\\\\t--\\\\t8\\\\tComp4\\\\t_\\\\tकर्मधारयः\\\\n8\\\\tकाय-\\\\t--\\\\t9\\\\tComp4\\\\t_\\\\tसप्तमी-तत्पुरुषः\\\\n9\\\\tप्रसृतान्\\\\t--\\\\t20\\\\tComp4\\\\t_\\\\tComp_root\\\\n10\\\\tअ-\\\\tअ-शेषान्\\\\t11\\\\tComp2\\\\t_\\\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\\\n11\\\\tशेषान्\\\\t--\\\\t20\\\\tComp2\\\\t_\\\\tComp_root\\\\n12\\\\tऔत्सुक्य-\\\\tऔत्सुक्य-मोह-अरति-दान्\\\\t13\\\\tComp4\\\\t_\\\\tइतरेतर-द्वन्द्वः\\\\n13\\\\tमोह-\\\\t--\\\\t14\\\\tComp4\\\\t_\\\\tइतरेतर-द्वन्द्वः\\\\n14\\\\tअ-\\\\t--\\\\t15\\\\tComp4\\\\t_\\\\tनञ्-तत्पुरुषः\\\\n15\\\\tरति-\\\\t--\\\\t16\\\\tComp4\\\\t_\\\\tComp_root\\\\n16\\\\tदान्\\\\t--\\\\t20\\\\tComp4\\\\t_\\\\tविशेषणम्\\\\n17\\\\tजघान\\\\tजघान\\\\t20\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n18\\\\t(यः)\\\\t(यः)\\\\t20\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n19\\\\t.\\\\t.\\\\t20\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n20\\\\tDUMMY\\\\t_\\\\t0\\\\tCompNo\\\\t_\\\\troot '}\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'][0], df['gold'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835292c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('अपूर्ववैद्याय नमः अस्तु तस्मै .',\n",
       " \"{'tokens': ['अ', 'पूर्व', 'वैद्याय', 'नमः', 'अस्तु', 'तस्मै', '.'], 'compounds': '1\\\\tअ-\\\\tअ-पूर्व-वैद्याय\\\\t2\\\\tComp3\\\\t_\\\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\\\n2\\\\tपूर्व-\\\\t--\\\\t3\\\\tComp3\\\\t_\\\\tकर्मधारयः\\\\n3\\\\tवैद्याय\\\\t--\\\\t8\\\\tComp3\\\\t_\\\\tComp_root\\\\n4\\\\tनमः\\\\tनमः\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n5\\\\tअस्तु\\\\tअस्तु\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n6\\\\tतस्मै\\\\tतस्मै\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n7\\\\t.\\\\t.\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n8\\\\tDUMMY\\\\t_\\\\t0\\\\tCompNo\\\\t_\\\\troot'}\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'][1], df['gold'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206ed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('अथ अतः आयुष्कामीयम् अध्यायम् व्याख्यास्यामः .',\n",
       " \"{'tokens': ['अथ', 'अतः', 'आयुष्कामीयम्', 'अध्यायम्', 'व्याख्यास्यामः', '.'], 'compounds': '1\\\\tअथ\\\\tअथ\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n2\\\\tअतः\\\\tअतः\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n3\\\\tआयुष्कामीयम्\\\\tआयुष्कामीयम्\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n4\\\\tअध्यायम्\\\\tअध्यायम्\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n5\\\\tव्याख्यास्यामः\\\\tव्याख्यास्यामः\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n6\\\\t.\\\\t.\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n7\\\\tDUMMY\\\\t_\\\\t0\\\\tCompNo\\\\t_\\\\troot'}\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'][2], df['gold'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efcc6cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tokenized length: 3300\n",
      "mean: 2174.7497490589712\n"
     ]
    }
   ],
   "source": [
    "SYSTEM = \"\"\"\n",
    "You are an expert in Sanskrit grammar, who identifies and classifies compounds in the given Sanskrit sentence. You will be given the original sentence. First break the sentence in compounds.\n",
    "Follow these rules strictly:\n",
    "1. Only use the following 4 compound types. Do not invent or include other types:\n",
    "    - Tatpurusha: An endocentric compound where the first element (the attributive) determines the second.\n",
    "    - Avyayibhava: An adverbial compound made of an indeclinable element and a noun, expressing an adverbial meaning.\n",
    "    - Dvandva: A copulative compound where two or more noun stems are joined by 'and'.\n",
    "    - Bahuvrihi: An exocentric compound that describes something by referring to its parts.\n",
    "2. The sentence may contain nested compounds or non-compounded words — handle appropriately.\n",
    "3. Maintain strict formatting and provide only the answer line. Do not include explanations.\n",
    "4. The start or end indexes must not exceed the number of words in the sentence.\n",
    "5. Answer in the devnagri script only, there shouldn't be any latin in the answer\n",
    "\n",
    "Text:\n",
    "{INPUT}\n",
    "\n",
    "Return strictly in JSON with keys:\n",
    "{\n",
    "  \"tokens\": [...] ,\n",
    "  \"compounds\":\"\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Tokenize by meaningful Sanskrit units.\n",
    "- span = inclusive of start index, exclusive of end index.\n",
    "- If multiple nested samāsa exist, include all.\n",
    "- If none, return empty lists for compounds.\n",
    "- Do not output anything outside JSON.\n",
    "- The compound data is in tab separated conllu format as given in example.\n",
    "\n",
    "\n",
    "Example 1:\n",
    "Input: रागआदिरोगान् सततअनुषक्तान् अशेषकायप्रसृतान् अशेषान् औत्सुक्यमोहअरतिदान् जघान (यः) .\n",
    "Output:\n",
    " {'tokens': ['राग', 'आदि', 'रोगान्', 'सतत', 'अनुषक्तान्', 'अ', 'शेष', 'काय', 'प्रसृतान्', 'अ', 'शेषान्', 'औत्सुक्य', 'मोह', 'अ', 'रति', 'दान्', 'जघान', '(यः)', '.'], 'compounds': '1\\\\tराग-\\\\tराग-आदि-रोगान्\\\\t2\\\\tComp3\\\\t_\\\\tबहुव्रीहिः\\\\n2\\\\tआदि-\\\\t--\\\\t3\\\\tComp3\\\\t_\\\\tकर्मधारयः\\\\n3\\\\tरोगान्\\\\t--\\\\t20\\\\tComp3\\\\t_\\\\tComp_root\\\\n4\\\\tसतत-\\\\tसतत-अनुषक्तान्\\\\t5\\\\tComp2\\\\t_\\\\tकर्मधारयः\\\\n5\\\\tअनुषक्तान्\\\\t--\\\\t20\\\\tComp2\\\\t_\\\\tComp_root\\\\n6\\\\tअ-\\\\tअ-शेष-काय-प्रसृतान्\\\\t7\\\\tComp4\\\\t_\\\\tनञ्-तत्पुरुषः\\\\n7\\\\tशेष-\\\\t--\\\\t8\\\\tComp4\\\\t_\\\\tकर्मधारयः\\\\n8\\\\tकाय-\\\\t--\\\\t9\\\\tComp4\\\\t_\\\\tसप्तमी-तत्पुरुषः\\\\n9\\\\tप्रसृतान्\\\\t--\\\\t20\\\\tComp4\\\\t_\\\\tComp_root\\\\n10\\\\tअ-\\\\tअ-शेषान्\\\\t11\\\\tComp2\\\\t_\\\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\\\n11\\\\tशेषान्\\\\t--\\\\t20\\\\tComp2\\\\t_\\\\tComp_root\\\\n12\\\\tऔत्सुक्य-\\\\tऔत्सुक्य-मोह-अरति-दान्\\\\t13\\\\tComp4\\\\t_\\\\tइतरेतर-द्वन्द्वः\\\\n13\\\\tमोह-\\\\t--\\\\t14\\\\tComp4\\\\t_\\\\tइतरेतर-द्वन्द्वः\\\\n14\\\\tअ-\\\\t--\\\\t15\\\\tComp4\\\\t_\\\\tनञ्-तत्पुरुषः\\\\n15\\\\tरति-\\\\t--\\\\t16\\\\tComp4\\\\t_\\\\tComp_root\\\\n16\\\\tदान्\\\\t--\\\\t20\\\\tComp4\\\\t_\\\\tविशेषणम्\\\\n17\\\\tजघान\\\\tजघान\\\\t20\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n18\\\\t(यः)\\\\t(यः)\\\\t20\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n19\\\\t.\\\\t.\\\\t20\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n20\\\\tDUMMY\\\\t_\\\\t0\\\\tCompNo\\\\t_\\\\troot '}\n",
    "   \n",
    "Example 2:\n",
    "Input: 'अपूर्ववैद्याय नमः अस्तु तस्मै .\n",
    "Output:\n",
    " {'tokens': ['अ', 'पूर्व', 'वैद्याय', 'नमः', 'अस्तु', 'तस्मै', '.'], \n",
    " 'compounds': '1\\\\tअ-\\\\tअ-पूर्व-वैद्याय\\\\t2\\\\tComp3\\\\t_\\\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\\\n2\\\\tपूर्व-\\\\t--\\\\t3\\\\tComp3\\\\t_\\\\tकर्मधारयः\\\\n3\\\\tवैद्याय\\\\t--\\\\t8\\\\tComp3\\\\t_\\\\tComp_root\\\\n4\\\\tनमः\\\\tनमः\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n5\\\\tअस्तु\\\\tअस्तु\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n6\\\\tतस्मै\\\\tतस्मै\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n7\\\\t.\\\\t.\\\\t8\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n8\\\\tDUMMY\\\\t_\\\\t0\\\\tCompNo\\\\t_\\\\troot'}\n",
    "    \n",
    "Input: अथ अतः आयुष्कामीयम् अध्यायम् व्याख्यास्यामः .\n",
    "Output:\n",
    " {'tokens': ['अथ', 'अतः', 'आयुष्कामीयम्', 'अध्यायम्', 'व्याख्यास्यामः', '.'], \n",
    " 'compounds': '1\\\\tअथ\\\\tअथ\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n2\\\\tअतः\\\\tअतः\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n3\\\\tआयुष्कामीयम्\\\\tआयुष्कामीयम्\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n4\\\\tअध्यायम्\\\\tअध्यायम्\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n5\\\\tव्याख्यास्यामः\\\\tव्याख्यास्यामः\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n6\\\\t.\\\\t.\\\\t7\\\\tCompNo\\\\t_\\\\tNo_rel\\\\n7\\\\tDUMMY\\\\t_\\\\t0\\\\tCompNo\\\\t_\\\\troot'}\n",
    "\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>Now analyze:{INPUT}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>Answer: {OUTPUT}<|eot_id|>\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "df = merged\n",
    "\n",
    "lens = []\n",
    "for i in range(len(df)):\n",
    "    inst = TEMPLATE.format(system=SYSTEM, INPUT=df[\"sentence\"][i], OUTPUT=\"\")\n",
    "    txt = inst + df[\"gold\"][i]\n",
    "    tokens = tokenizer(txt, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
    "    lens.append(len(tokens))\n",
    "\n",
    "print(\"max tokenized length:\", max(lens))\n",
    "print(\"mean:\", sum(lens)/len(lens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64191a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "import json\n",
    "# ---------- configs ----------\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "TRAIN_CSV = \"/home/shivraj-pg/DEPNECT/conllu-style-csv/without-context-coarse-train.csv\"\n",
    "OUT_DIR = \"/home/shivraj-pg/DEPNECT/OUT_gemma4B_conllu\"\n",
    "\n",
    "MAX_SEQ = 3500\n",
    "R, ALPHA = 64, 128\n",
    "DROPOUT = 0.1\n",
    "LR = 1e-5\n",
    "BATCH = 8\n",
    "GRAD_ACC = 16\n",
    "EPOCHS = 50\n",
    "SAVE_STEPS = 100\n",
    "LOG_STEPS = 10\n",
    "\n",
    "SYSTEM = \"\"\"\n",
    "You are an expert in Sanskrit grammar, who identifies and classifies compounds in the given Sanskrit sentence. You will be given the original sentence. First break the sentence in compounds.\n",
    "Follow these rules strictly:\n",
    "1. Only use the following 4 compound types. Do not invent or include other types:\n",
    "    - Tatpurusha: An endocentric compound where the first element (the attributive) determines the second.\n",
    "    - Avyayibhava: An adverbial compound made of an indeclinable element and a noun, expressing an adverbial meaning.\n",
    "    - Dvandva: A copulative compound where two or more noun stems are joined by 'and'.\n",
    "    - Bahuvrihi: An exocentric compound that describes something by referring to its parts.\n",
    "2. The sentence may contain nested compounds or non-compounded words — handle appropriately.\n",
    "3. Maintain strict formatting and provide only the answer line. Do not include explanations.\n",
    "4. The start or end indexes must not exceed the number of words in the sentence.\n",
    "5. Answer in the devnagri script only, there shouldn't be any latin in the answer\n",
    "\n",
    "Return strictly in JSON with keys:\n",
    "{\n",
    "  \"tokens\": [...] ,\n",
    "  \"compounds\":\"\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Tokenize by meaningful Sanskrit units.\n",
    "- span = inclusive of start index, exclusive of end index.\n",
    "- If multiple nested samāsa exist, include all.\n",
    "- If none, return empty lists for compounds.\n",
    "- Do not output anything outside JSON.\n",
    "- The compound data is in tab separated conllu format as given in example.\n",
    "\"\"\"\n",
    "\n",
    "ex1_input = \"रागआदिरोगान् सततअनुषक्तान् अशेषकायप्रसृतान् अशेषान् औत्सुक्यमोहअरतिदान् जघान (यः) .\"\n",
    "\n",
    "ex1_output = {\n",
    "    \"tokens\": [\"राग\", \"आदि\", \"रोगान्\", \"सतत\", \"अनुषक्तान्\", \"अ\", \"शेष\", \"काय\", \"प्रसृतान्\", \"अ\", \"शेषान्\", \"औत्सुक्य\", \"मोह\", \"अ\", \"रति\", \"दान्\", \"जघान\", \"(यः)\", \".\"],\n",
    "    \"compounds\": \"1\\tराग-\\tराग-आदि-रोगान्\\t2\\tComp3\\t_\\tबहुव्रीहिः\\n2\\tआदि-\\t--\\t3\\tComp3\\t_\\tकर्मधारयः\\n3\\tरोगान्\\t--\\t20\\tComp3\\t_\\tComp_root\\n4\\tसतत-\\tसतत-अनुषक्तान्\\t5\\tComp2\\t_\\tकर्मधारयः\\n5\\tअनुषक्तान्\\t--\\t20\\tComp2\\t_\\tComp_root\\n6\\tअ-\\tअ-शेष-काय-प्रसृतान्\\t7\\tComp4\\t_\\tनञ्-तत्पुरुषः\\n7\\tशेष-\\t--\\t8\\tComp4\\t_\\tकर्मधारयः\\n8\\tकाय-\\t--\\t9\\tComp4\\t_\\tसप्तमी-तत्पुरुषः\\n9\\tप्रसृतान्\\t--\\t20\\tComp4\\t_\\tComp_root\\n10\\tअ-\\tअ-शेषान्\\t11\\tComp2\\t_\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\n11\\tशेषान्\\t--\\t20\\tComp2\\t_\\tComp_root\\n12\\tऔत्सुक्य-\\tऔत्सुक्य-मोह-अरति-दान्\\t13\\tComp4\\t_\\tइतरेतर-द्वन्द्वः\\n13\\tमोह-\\t--\\t14\\tComp4\\t_\\tइतरेतर-द्वन्द्वः\\n14\\tअ-\\t--\\t15\\tComp4\\t_\\tनञ्-तत्पुरुषः\\n15\\tरति-\\t--\\t16\\tComp4\\t_\\tComp_root\\n16\\tदान्\\t--\\t20\\tComp4\\t_\\tविशेषणम्\\n17\\tजघान\\tजघान\\t20\\tCompNo\\t_\\tNo_rel\\n18\\t(यः)\\t(यः)\\t20\\tCompNo\\t_\\tNo_rel\\n19\\t.\\t.\\t20\\tCompNo\\t_\\tNo_rel\\n20\\tDUMMY\\t_\\t0\\tCompNo\\t_\\troot\"\n",
    "}\n",
    "ex2_input = \"अपूर्ववैद्याय नमः अस्तु तस्मै .\"\n",
    "\n",
    "ex2_output = {\n",
    "    \"tokens\": [\"अ\", \"पूर्व\", \"वैद्याय\", \"नमः\", \"अस्तु\", \"तस्मै\", \".\"],\n",
    "    \"compounds\": \"1\\tअ-\\tअ-पूर्व-वैद्याय\\t2\\tComp3\\t_\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\n2\\tपूर्व-\\t--\\t3\\tComp3\\t_\\tकर्मधारयः\\n3\\tवैद्याय\\t--\\t8\\tComp3\\t_\\tComp_root\\n4\\tनमः\\tनमः\\t8\\tCompNo\\t_\\tNo_rel\\n5\\tअस्तु\\tअस्तु\\t8\\tCompNo\\t_\\tNo_rel\\n6\\tतस्मै\\tतस्मै\\t8\\tCompNo\\t_\\tNo_rel\\n7\\t.\\t.\\t8\\tCompNo\\t_\\tNo_rel\\n8\\tDUMMY\\t_\\t0\\tCompNo\\t_\\troot\"\n",
    "}\n",
    "ex3_input = \"अथ अतः आयुष्कामीयम् अध्यायम् व्याख्यास्यामः .\"\n",
    "\n",
    "ex3_output = {\n",
    "    \"tokens\": [\"अथ\", \"अतः\", \"आयुष्कामीयम्\", \"अध्यायम्\", \"व्याख्यास्यामः\", \".\"],\n",
    "    \"compounds\": \"1\\tअथ\\tअथ\\t7\\tCompNo\\t_\\tNo_rel\\n2\\tअतः\\tअतः\\t7\\tCompNo\\t_\\tNo_rel\\n3\\tआयुष्कामीयम्\\tआयुष्कामीयम्\\t7\\tCompNo\\t_\\tNo_rel\\n4\\tअध्यायम्\\tअध्यायम्\\t7\\tCompNo\\t_\\tNo_rel\\n5\\tव्याख्यास्यामः\\tव्याख्यास्यामः\\t7\\tCompNo\\t_\\tNo_rel\\n6\\t.\\t.\\t7\\tCompNo\\t_\\tNo_rel\\n7\\tDUMMY\\t_\\t0\\tCompNo\\t_\\troot\"\n",
    "}\n",
    "\n",
    "\n",
    "DEMO_EXAMPLES = [\n",
    "    (ex1_input, json.dumps(ex1_output, ensure_ascii=False)),\n",
    "    (ex2_input, json.dumps(ex2_output, ensure_ascii=False)),\n",
    "    (ex3_input, json.dumps(ex3_output, ensure_ascii=False)),\n",
    "]\n",
    "\n",
    "# ---------- dataset prep ----------\n",
    "\n",
    "\n",
    "def create_demo_block(demos):\n",
    "    blocks = []\n",
    "    for i, (inp, out_json) in enumerate(demos, 1):\n",
    "        block = (\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "            f\"Example {i} Input:\\n{inp}\\n\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "            f\"Example {i} Output:\\n{out_json}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "\n",
    "demo_block = create_demo_block(DEMO_EXAMPLES)\n",
    "\n",
    "\n",
    "def build_prompt(system_text: str, demo_block: str, target_sentence: str) -> str:\n",
    "    # This returns only the prompt (no target gold).\n",
    "    return (\n",
    "        \"<|begin_of_text|>\\n\"\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"{system_text}\\n\\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        f\"{demo_block}\\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        \"Now analyze this sentence:\\n\"\n",
    "        f\"{target_sentence}\\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def csv_to_ds(path):\n",
    "    df = pd.read_csv(path)[[\"sentence\", \"gold\"]].dropna()\n",
    "\n",
    "    texts = []\n",
    "    for _, r in df.iterrows():\n",
    "        prompt = build_prompt(SYSTEM, demo_block, str(r[\"sentence\"]).strip())\n",
    "        # gold must be strict JSON string already in your dataset\n",
    "        gold = str(r[\"gold\"]).strip()\n",
    "        # final string = prompt + gold (assistant completion)\n",
    "        full_text = prompt + gold\n",
    "        texts.append(full_text)\n",
    "\n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1dec816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11000/11000 [00:16<00:00, 669.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for train:\n",
      " Count: 11000\n",
      " Max length: 1864\n",
      " Min length: 1780\n",
      " Mean length: 1801.97\n",
      " 95th percentile: 1828\n",
      " 99th percentile: 1842\n",
      "\n",
      "Processing dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 703.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for dev:\n",
      " Count: 2000\n",
      " Max length: 1851\n",
      " Min length: 1781\n",
      " Mean length: 1802.14\n",
      " 95th percentile: 1827\n",
      " 99th percentile: 1841\n",
      "\n",
      "Global max length: 1864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- paths ----------\n",
    "TRAIN_CSV = \"/home/shivraj-pg/DEPNECT/conllu-style-csv/without-context-coarse-train.csv\"\n",
    "DEV_CSV   = \"/home/shivraj-pg/DEPNECT/conllu-style-csv/without-context-coarse-dev.csv\"\n",
    "\n",
    "# ---------- use your real SYSTEM, build_prompt, and demo_block ----------\n",
    "# Make sure these are already defined in your environment.\n",
    "# I am assuming they are available exactly as you showed previously.\n",
    "\n",
    "# Example:\n",
    "# SYSTEM\n",
    "# demo_block\n",
    "# build_prompt(system_text, demo_block, target_sentence)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "def compute_lengths(csv_path, name):\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)[[\"sentence\"]].dropna()\n",
    "    lengths = []\n",
    "\n",
    "    for s in tqdm(df[\"sentence\"], total=len(df)):\n",
    "        prompt = build_prompt(\n",
    "            SYSTEM,\n",
    "            demo_block,\n",
    "            str(s).strip()\n",
    "        )\n",
    "        ids = tok(prompt, return_attention_mask=False, add_special_tokens=False)[\"input_ids\"]\n",
    "        lengths.append(len(ids))\n",
    "\n",
    "    print(f\"Stats for {name}:\")\n",
    "    print(f\" Count: {len(lengths)}\")\n",
    "    print(f\" Max length: {max(lengths)}\")\n",
    "    print(f\" Min length: {min(lengths)}\")\n",
    "    print(f\" Mean length: {sum(lengths)/len(lengths):.2f}\")\n",
    "    print(f\" 95th percentile: {sorted(lengths)[int(0.95*len(lengths))]}\")\n",
    "    print(f\" 99th percentile: {sorted(lengths)[int(0.99*len(lengths))]}\")\n",
    "    return lengths\n",
    "\n",
    "\n",
    "train_lengths = compute_lengths(TRAIN_CSV, \"train\")\n",
    "dev_lengths = compute_lengths(DEV_CSV, \"dev\")\n",
    "\n",
    "print(\"\\nGlobal max length:\", max(max(train_lengths), max(dev_lengths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b24b4106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FULL PROMPT INPUT TO MODEL =====\n",
      "<|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an expert in Sanskrit grammar, who identifies and classifies compounds in the given Sanskrit sentence. You will be given the original sentence. First break the sentence in compounds.\n",
      "Follow these rules strictly:\n",
      "1. Only use the following 4 compound types. Do not invent or include other types:\n",
      "    - Tatpurusha: An endocentric compound where the first element (the attributive) determines the second.\n",
      "    - Avyayibhava: An adverbial compound made of an indeclinable element and a noun, expressing an adverbial meaning.\n",
      "    - Dvandva: A copulative compound where two or more noun stems are joined by 'and'.\n",
      "    - Bahuvrihi: An exocentric compound that describes something by referring to its parts.\n",
      "2. The sentence may contain nested compounds or non-compounded words — handle appropriately.\n",
      "3. Maintain strict formatting and provide only the answer line. Do not include explanations.\n",
      "4. The start or end indexes must not exceed the number of words in the sentence.\n",
      "5. Answer in the devnagri script only, there shouldn't be any latin in the answer\n",
      "\n",
      "Return strictly in JSON with keys:\n",
      "{\n",
      "  \"tokens\": [...] ,\n",
      "  \"compounds\":\"\"\n",
      "}\n",
      "\n",
      "Rules:\n",
      "- Tokenize by meaningful Sanskrit units.\n",
      "- span = inclusive of start index, exclusive of end index.\n",
      "- If multiple nested samāsa exist, include all.\n",
      "- If none, return empty lists for compounds.\n",
      "- Do not output anything outside JSON.\n",
      "- The compound data is in tab separated conllu format as given in example.\n",
      "\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Example 1 Input:\n",
      "रागआदिरोगान् सततअनुषक्तान् अशेषकायप्रसृतान् अशेषान् औत्सुक्यमोहअरतिदान् जघान (यः) .\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Example 1 Output:\n",
      "{\"tokens\": [\"राग\", \"आदि\", \"रोगान्\", \"सतत\", \"अनुषक्तान्\", \"अ\", \"शेष\", \"काय\", \"प्रसृतान्\", \"अ\", \"शेषान्\", \"औत्सुक्य\", \"मोह\", \"अ\", \"रति\", \"दान्\", \"जघान\", \"(यः)\", \".\"], \"compounds\": \"1\\tराग-\\tराग-आदि-रोगान्\\t2\\tComp3\\t_\\tबहुव्रीहिः\\n2\\tआदि-\\t--\\t3\\tComp3\\t_\\tकर्मधारयः\\n3\\tरोगान्\\t--\\t20\\tComp3\\t_\\tComp_root\\n4\\tसतत-\\tसतत-अनुषक्तान्\\t5\\tComp2\\t_\\tकर्मधारयः\\n5\\tअनुषक्तान्\\t--\\t20\\tComp2\\t_\\tComp_root\\n6\\tअ-\\tअ-शेष-काय-प्रसृतान्\\t7\\tComp4\\t_\\tनञ्-तत्पुरुषः\\n7\\tशेष-\\t--\\t8\\tComp4\\t_\\tकर्मधारयः\\n8\\tकाय-\\t--\\t9\\tComp4\\t_\\tसप्तमी-तत्पुरुषः\\n9\\tप्रसृतान्\\t--\\t20\\tComp4\\t_\\tComp_root\\n10\\tअ-\\tअ-शेषान्\\t11\\tComp2\\t_\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\n11\\tशेषान्\\t--\\t20\\tComp2\\t_\\tComp_root\\n12\\tऔत्सुक्य-\\tऔत्सुक्य-मोह-अरति-दान्\\t13\\tComp4\\t_\\tइतरेतर-द्वन्द्वः\\n13\\tमोह-\\t--\\t14\\tComp4\\t_\\tइतरेतर-द्वन्द्वः\\n14\\tअ-\\t--\\t15\\tComp4\\t_\\tनञ्-तत्पुरुषः\\n15\\tरति-\\t--\\t16\\tComp4\\t_\\tComp_root\\n16\\tदान्\\t--\\t20\\tComp4\\t_\\tविशेषणम्\\n17\\tजघान\\tजघान\\t20\\tCompNo\\t_\\tNo_rel\\n18\\t(यः)\\t(यः)\\t20\\tCompNo\\t_\\tNo_rel\\n19\\t.\\t.\\t20\\tCompNo\\t_\\tNo_rel\\n20\\tDUMMY\\t_\\t0\\tCompNo\\t_\\troot\"}\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Example 2 Input:\n",
      "अपूर्ववैद्याय नमः अस्तु तस्मै .\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Example 2 Output:\n",
      "{\"tokens\": [\"अ\", \"पूर्व\", \"वैद्याय\", \"नमः\", \"अस्तु\", \"तस्मै\", \".\"], \"compounds\": \"1\\tअ-\\tअ-पूर्व-वैद्याय\\t2\\tComp3\\t_\\tअस्त्यर्थ-मध्यमपदलोपी(नञ्)-बहुव्रीहिः\\n2\\tपूर्व-\\t--\\t3\\tComp3\\t_\\tकर्मधारयः\\n3\\tवैद्याय\\t--\\t8\\tComp3\\t_\\tComp_root\\n4\\tनमः\\tनमः\\t8\\tCompNo\\t_\\tNo_rel\\n5\\tअस्तु\\tअस्तु\\t8\\tCompNo\\t_\\tNo_rel\\n6\\tतस्मै\\tतस्मै\\t8\\tCompNo\\t_\\tNo_rel\\n7\\t.\\t.\\t8\\tCompNo\\t_\\tNo_rel\\n8\\tDUMMY\\t_\\t0\\tCompNo\\t_\\troot\"}\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Example 3 Input:\n",
      "अथ अतः आयुष्कामीयम् अध्यायम् व्याख्यास्यामः .\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Example 3 Output:\n",
      "{\"tokens\": [\"अथ\", \"अतः\", \"आयुष्कामीयम्\", \"अध्यायम्\", \"व्याख्यास्यामः\", \".\"], \"compounds\": \"1\\tअथ\\tअथ\\t7\\tCompNo\\t_\\tNo_rel\\n2\\tअतः\\tअतः\\t7\\tCompNo\\t_\\tNo_rel\\n3\\tआयुष्कामीयम्\\tआयुष्कामीयम्\\t7\\tCompNo\\t_\\tNo_rel\\n4\\tअध्यायम्\\tअध्यायम्\\t7\\tCompNo\\t_\\tNo_rel\\n5\\tव्याख्यास्यामः\\tव्याख्यास्यामः\\t7\\tCompNo\\t_\\tNo_rel\\n6\\t.\\t.\\t7\\tCompNo\\t_\\tNo_rel\\n7\\tDUMMY\\t_\\t0\\tCompNo\\t_\\troot\"}\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Now analyze this sentence:\n",
      "स सर्षपं तुम्बुरु धान्य वन्यं चण्डां च चूर्णानि समानि कुर्यात् DUMMY\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "{\"tokens\": [\"स\", \"सर्षपं\", \"तुम्बुरु\", \"धान्य\", \"वन्यं\", \"चण्डां\", \"च\", \"चूर्णानि\", \"समानि\", \"कुर्यात्\", \"DUMMY\"], \"compounds\": \"1\\tस\\t_\\tComp2_Start\\t_\\t_\\t2\\tBahuvrihi\\t2\\t11\\n2\\tसर्षपं\\t_\\tComp2_End\\t_\\t_\\t11\\tComp_root\\t11\\t11\\n3\\tतुम्बुरु\\t_\\tComp3_Start\\t_\\t_\\t5\\tDvandva\\t5\\t11\\n4\\tधान्य\\t_\\tComp3_Middle\\t_\\t_\\t5\\tDvandva\\t5\\t11\\n5\\tवन्यं\\t_\\tComp3_End\\t_\\t_\\t11\\tComp_root\\t11\\t11\\n6\\tचण्डां\\t_\\tCompNo\\t_\\t_\\t11\\tNo_rel\\t11\\t11\\n7\\tच\\t_\\tCompNo\\t_\\t_\\t11\\tNo_rel\\t11\\t11\\n8\\tचूर्णानि\\t_\\tCompNo\\t_\\t_\\t11\\tNo_rel\\t11\\t11\\n9\\tसमानि\\t_\\tCompNo\\t_\\t_\\t11\\tNo_rel\\t11\\t11\\n10\\tकुर्यात्\\t_\\tCompNo\\t_\\t_\\t11\\tNo_rel\\t11\\t11\\n11\\tDUMMY\\t_\\tCompNo\\t_\\t_\\t0\\troot\\t0\\t0\"}\n",
      "======================================\n",
      "Token count: 2240\n"
     ]
    }
   ],
   "source": [
    "# Pick a sample index from your training dataset\n",
    "sample_idx = 0\n",
    "\n",
    "# Load the dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV)[[\"sentence\", \"gold\"]].dropna()\n",
    "\n",
    "# Get the specific row\n",
    "sentence = df.iloc[sample_idx][\"sentence\"]\n",
    "gold = df.iloc[sample_idx][\"gold\"]\n",
    "\n",
    "# Build prompt exactly like your csv_to_ds()\n",
    "prompt_only = build_prompt(\n",
    "    SYSTEM,\n",
    "    demo_block,\n",
    "    str(sentence).strip()\n",
    ")\n",
    "\n",
    "full_input = prompt_only + str(gold).strip()\n",
    "\n",
    "print(\"===== FULL PROMPT INPUT TO MODEL =====\")\n",
    "print(full_input)\n",
    "print(\"======================================\")\n",
    "\n",
    "# If you also want to see token count:\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "print(\"Token count:\", len(tok(full_input)[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2906d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depnect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
